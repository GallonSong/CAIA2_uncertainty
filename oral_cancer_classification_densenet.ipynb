{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"oral_cancer_classification_densenet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOXNNdsh6lcNU3MIMKY3efC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hrG8PikUqGO4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"d438ff55-70be-4ac1-8d2a-edb50170de52","executionInfo":{"status":"ok","timestamp":1584869148460,"user_tz":-60,"elapsed":30851,"user":{"displayName":"Gallon Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlSi83hN0kVpWtkUrw6LIfxrBGxYoQ7JSm_ntF=s64","userId":"17401631192895020422"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4u9BrBj2DVHT","colab_type":"code","colab":{}},"source":["a = []\n","while(1):\n","    a.append('1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLkyPiNXIKy1","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"993hjuuCrEs3","colab_type":"code","outputId":"93dcb7bf-a85f-4f24-ca8b-9da5be48dd30","executionInfo":{"status":"ok","timestamp":1584869160846,"user_tz":-60,"elapsed":3705,"user":{"displayName":"Gallon Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlSi83hN0kVpWtkUrw6LIfxrBGxYoQ7JSm_ntF=s64","userId":"17401631192895020422"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('GPU device not found')\n","else:\n","  print('Found GPU at: {}'.format(device_name))\n","\n","from tensorflow.keras.applications.densenet import DenseNet121\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","print(\"AUTOTUNE: \", AUTOTUNE)\n","\n","import IPython.display as display\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","Num GPUs Available:  1\n","Found GPU at: /device:GPU:0\n","AUTOTUNE:  -1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nocIPCcL0wC-","colab_type":"code","outputId":"fafe2a58-f5b9-4676-be24-a7bc79926eb3","executionInfo":{"status":"ok","timestamp":1584869164650,"user_tz":-60,"elapsed":3242,"user":{"displayName":"Gallon Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlSi83hN0kVpWtkUrw6LIfxrBGxYoQ7JSm_ntF=s64","userId":"17401631192895020422"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pathlib\n","project_dir = \"/content/drive/My Drive/master_uppsala/p3_computer-assisted_image_analysis_2_project/\"\n","oral_cancer_dataset_dir = project_dir + \"OralCancer_DataSet3/\"\n","train_dir = pathlib.Path(oral_cancer_dataset_dir + \"train/\")\n","\n","CLASS_NAMES = np.array([item.name for item in train_dir.glob('*')])\n","print(CLASS_NAMES)\n","total_train = 73303  # len(list(train_dir.glob('*/*/*.jpg')))  # 73303\n","total_test = 55514  # len(list(test_dir.glob('*/*/*.jpg')))  # 55514\n","\n","BATCH_SIZE = 128\n","epochs = 30\n","IMG_HEIGHT = 80\n","IMG_WIDTH = 80\n","\n","# from IPython.display import display\n","# image_example_dir = str(list(train_dir.glob('Cancer/000/*.jpg'))[0])\n","# display(Image.open(image_example_dir))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['Cancer' 'Healthy']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oC_od-Lf09Xe","colab_type":"code","colab":{}},"source":["train_record_file = oral_cancer_dataset_dir + 'train_images.tfrecords'\n","test_record_file = oral_cancer_dataset_dir + 'test_images.tfrecords'\n","\n","train_image_dataset = tf.data.TFRecordDataset(train_record_file)\n","test_image_dataset = tf.data.TFRecordDataset(test_record_file)\n","# Create a dictionary describing the features.\n","image_feature_description = {\n","    'label': tf.io.FixedLenFeature([], tf.int64),\n","    'image_raw': tf.io.FixedLenFeature([], tf.string),\n","}\n","\n","def _parse_image_function(example_proto):\n","  # Parse the input tf.Example proto using the dictionary above.\n","  return tf.io.parse_single_example(example_proto, image_feature_description)\n","\n","train_ds = train_image_dataset.map(_parse_image_function)\n","test_ds = test_image_dataset.map(_parse_image_function)\n","\n","# for a in test_ds.take(1):\n","#   raw_image = a['image_raw'].numpy()\n","#   label = a['label'].numpy()\n","#   print(label)\n","#   display.display(display.Image(raw_image))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQmaX2HIelgY","colab_type":"code","colab":{}},"source":["def decode_img_label(x):\n","  def _decode_img(img):\n","    img = tf.image.decode_jpeg(img, channels=3)  # convert the compressed string to a 3D uint8 tensor\n","    return tf.image.convert_image_dtype(img, tf.float32)  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n","  \n","  img = _decode_img(x['image_raw'])\n","  label = (CLASS_NAMES == CLASS_NAMES[0]).astype(int) == x['label']\n","  return img, label\n","\n","train_labeled_ds = train_ds.map(decode_img_label, num_parallel_calls=AUTOTUNE)\n","test_labeled_ds = test_ds.map(decode_img_label, num_parallel_calls=AUTOTUNE)\n","\n","# for b in test_labeled_ds.take(1):\n","#   print(b[0].numpy())\n","#   print(b[1].numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjSikbaXw00D","colab_type":"code","colab":{}},"source":["# train_labeled_ds = train_labeled_ds.cache()\n","# test_labeled_ds = test_labeled_ds.cache()f\n","\n","def augment(ds):\n","  lr_flip_ds = ds.map(lambda x,y: (tf.image.flip_left_right(x),y), num_parallel_calls=AUTOTUNE)\n","  ud_flip_ds = ds.map(lambda x,y: (tf.image.flip_up_down(x),y), num_parallel_calls=AUTOTUNE)\n","  rot90_ds = ds.map(lambda x,y: (tf.image.rot90(x),y), num_parallel_calls=AUTOTUNE)\n","  rot180_ds = ds.map(lambda x,y: (tf.image.rot90(x, k=2),y), num_parallel_calls=AUTOTUNE)\n","  rot270_ds = ds.map(lambda x,y: (tf.image.rot90(x, k=3),y), num_parallel_calls=AUTOTUNE)\n","  lr_flip_rot90_ds = lr_flip_ds.map(lambda x,y: (tf.image.rot90(x),y), num_parallel_calls=AUTOTUNE)\n","  lr_flip_rot270_ds = lr_flip_ds.map(lambda x,y: (tf.image.rot90(x, k=3),y), num_parallel_calls=AUTOTUNE)\n","  return ds.concatenate(lr_flip_ds).concatenate(ud_flip_ds).concatenate(rot90_ds).concatenate(rot180_ds).concatenate(rot270_ds).concatenate(lr_flip_rot90_ds).concatenate(lr_flip_rot270_ds)\n","\n","train_augmented_ds = augment(train_labeled_ds)\n","test_augmented_ds = augment(test_labeled_ds)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuuwTeGq1IXl","colab_type":"code","colab":{}},"source":["def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n","  # This is a small dataset, only load it once, and keep it in memory.\n","  # use `.cache(filename)` to cache preprocessing work for datasets that don't fit in memory.\n","  if cache:\n","    if isinstance(cache, str):\n","      ds = ds.cache(cache)\n","    else:\n","      ds = ds.cache()\n","  # ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n","  # ds = ds.repeat()  # Repeat forever\n","  ds = ds.batch(1)\n","  ds = ds.prefetch(buffer_size=AUTOTUNE)  # `prefetch` lets the dataset fetch batches in the background while the model is training.\n","  return ds\n","\n","train_model_ds = prepare_for_training(train_augmented_ds, cache=False)\n","test_model_ds = prepare_for_training(test_augmented_ds, cache=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLmncVVk1N8o","colab_type":"code","colab":{}},"source":["def show_batch(image_batch, label_batch):\n","  plt.figure(figsize=(10,10))\n","  for n in range(25):\n","      ax = plt.subplot(5,5,n+1)\n","      plt.imshow(image_batch[n])\n","      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n","      plt.axis('off')\n","\n","image_batch, label_batch = next(iter(train_model_ds))\n","show_batch(image_batch.numpy(), label_batch.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLX8JQPJsGGi","colab_type":"code","outputId":"2065a5f7-aab8-4437-d706-40160076b6af","executionInfo":{"status":"error","timestamp":1583918233577,"user_tz":-60,"elapsed":24362,"user":{"displayName":"Gallon Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlSi83hN0kVpWtkUrw6LIfxrBGxYoQ7JSm_ntF=s64","userId":"17401631192895020422"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["dn_model = DenseNet121(\n","    include_top=False, weights='imagenet', input_tensor=None, \n","    input_shape=None, pooling=None)\n","\n","# dimensions of our images.\n","img_width, img_height = 80, 80\n","\n","top_model_weights_path = project_dir + 'densenet_features/bottleneck_fc_model.h5'\n","\n","# the predict_generator method returns the output of a model, given a generator that yields batches of numpy data\n","# bottleneck_features_train = dn_model.predict(train_model_ds, steps=total_train*8, verbose=1)\n","# densenet_train_feature_file = project_dir + 'densenet_features/bottleneck_features_train.npy'\n","# np.save(open(densenet_train_feature_file, 'wb'), bottleneck_features_train)\n","\n","bottleneck_features_validation = dn_model.predict(test_model_ds, steps=total_test*8, verbose=1)\n","# densenet_validation_feature_file = project_dir + 'densenet_features/bottleneck_features_validation.npy'\n","# np.save(open(densenet_validation_feature_file, 'wb'), bottleneck_features_validation)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n","29089792/29084464 [==============================] - 0s 0us/step\n"," 22792/444112 [>.............................] - ETA: 2:09:25"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jGmOFxmatzAs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"4815cdeb-c5cd-4ce1-a761-8cde0e4d93c2","executionInfo":{"status":"error","timestamp":1584726724949,"user_tz":-60,"elapsed":223354,"user":{"displayName":"Gallon Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlSi83hN0kVpWtkUrw6LIfxrBGxYoQ7JSm_ntF=s64","userId":"17401631192895020422"}}},"source":["densenet_train_feature_file = project_dir + 'densenet_features/bottleneck_features_train.npy'\n","# densenet_validation_feature_file = project_dir + 'densenet_features/bottleneck_features_validation.npy'\n","\n","nb_train_canser_samples, nb_train_healthy_samples = 22453, 50850\n","nb_validation_canser_samples, nb_validation_healthy_samples = 20322, 35192\n","\n","train_data = np.load(open(densenet_train_feature_file, 'rb'))\n","# the features were saved in order, so recreating the labels is easy\n","train_labels = np.array(([0] * nb_train_canser_samples + [1] * nb_train_healthy_samples)*8)\n","\n","# validation_data = np.load(open(densenet_validation_feature_file, 'rb'))\n","validation_data = bottleneck_features_validation\n","validation_labels = np.array(([0] * nb_validation_canser_samples + [1] * nb_validation_healthy_samples)*8)"],"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e426cf5a2df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb_train_canser_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb_train_healthy_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdensenet_validation_feature_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb_validation_canser_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb_validation_healthy_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/master_uppsala/p3_computer-assisted_image_analysis_2_project/densenet_features/bottleneck_features_validation.npy'"]}]},{"cell_type":"code","metadata":{"id":"_CVd1Cbht7ks","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Flatten(input_shape=train_data.shape[1:]))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","model.fit(train_data, train_labels,\n","          epochs=epochs,\n","          batch_size=BATCH_SIZE,\n","          validation_data=(validation_data, validation_labels))\n","model.save_weights(project_dir + '/densenet_features/bottleneck_fc_model.h5')"],"execution_count":0,"outputs":[]}]}